{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mEC8iOUXf_sI"
      },
      "outputs": [],
      "source": [
        "#@title Import Statements\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "import matplotlib.image as mpimg \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import OrderedDict \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models,transforms\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNrv7hkmFHnY"
      },
      "source": [
        "# Data Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLtvm25PhlSq",
        "outputId": "5c871630-0c50-4d8e-bf8d-c473ef87b2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QmM4n8K24kAAqnmIcDl7tMWSAmBJaLcN\n",
            "To: /content/HAMDATA.zip\n",
            "100% 2.77G/2.77G [00:14<00:00, 185MB/s]\n"
          ]
        }
      ],
      "source": [
        "# get the dataset \n",
        "!gdown --id 1QmM4n8K24kAAqnmIcDl7tMWSAmBJaLcN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bibsDzKyilpw"
      },
      "outputs": [],
      "source": [
        "directory = \"ham_data\"\n",
        "# create a directory on this vm\n",
        "new_directory_path = os.path.join(os.getcwd(), directory)\n",
        "os.mkdir(new_directory_path)\n",
        "#extract the dataset into this new directory\n",
        "with zipfile.ZipFile('HAMDATA.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(new_directory_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsGiqrfef_sM"
      },
      "outputs": [],
      "source": [
        "datapath = os.getcwd() + \"/ham_data/data\"\n",
        "# a number less than 1 \n",
        "#change the fraction of data sampled to help memory and run time\n",
        "# at 1 has a 3-4 hour run time\n",
        "sample_fraction = 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLdLidpj6PYU"
      },
      "source": [
        "# DATA AND FEATURE MANIPULATION AND FORMATTING SECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYKBqvX1f_sN"
      },
      "outputs": [],
      "source": [
        "#underlying condtion possible values will be classification output class\n",
        "#the first three are not deadly the last 4 are\n",
        "# dx in dataframe and dataset\n",
        "outputClasses = np.array(['df','bkl','nv', 'vasc', 'mel', 'akiec', 'bcc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B2eHGuBf_sO"
      },
      "outputs": [],
      "source": [
        "#gets a dataframe with a sample of the full dataset\n",
        "def getSampleOfData(sample_fraction):\n",
        "    #Get MetaData in dataframe\n",
        "    fullhamdf = pd.read_csv(datapath + '/HAM10000_metadata.csv')\n",
        "    #drop the useless lesion id and the diagnosis validation type information after dropping duplicates in dataset\n",
        "    fullhamdf = fullhamdf.drop_duplicates(subset='lesion_id', keep='first').drop([\"lesion_id\", \"dx_type\"], axis=1).dropna()\n",
        "    #Random Sample for run time\n",
        "    return fullhamdf.sample(frac=sample_fraction, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG2-JvK_f_sO"
      },
      "outputs": [],
      "source": [
        "# gets a dataframe with a sample of the dataset with all features in numerical or vector format\n",
        "def getHamDataFrame(sample_fraction):\n",
        "    #gets a dataframe with a sample of the dataset\n",
        "    hamdf = getSampleOfData(sample_fraction)\n",
        "    \n",
        "    # used to make male be 1 female anything else 0\n",
        "    def binarySex(sex):\n",
        "        return int(sex == \"male\")\n",
        "    hamdf[\"sex\"] = hamdf[\"sex\"].map(binarySex)\n",
        "    hamdf.head\n",
        "    locClasses = hamdf[\"localization\"].unique()\n",
        "\n",
        "    #used to get diffrent locations as diffrent binary classes\n",
        "    def binaryLoc(location):\n",
        "        return int(curloc == location)\n",
        "    for loc in locClasses:\n",
        "        hamdf[loc] = np.zeros(len(hamdf))\n",
        "        curloc = loc \n",
        "        hamdf[loc] = hamdf[\"localization\"].map(binaryLoc)\n",
        "\n",
        "    #used to get an compressed image from data as a vector\n",
        "    def getHamImageAsVector(image_id):\n",
        "        img = Image.open(datapath + \"/HAM10000_images/\" + image_id +\".jpg\")\n",
        "        return np.array(img.resize((32,32),Image.ANTIALIAS).getdata())\n",
        "    hamdf[\"image_data\"] = hamdf[\"image_id\"].map(getHamImageAsVector)\n",
        "\n",
        "    #get output classes as numerical values\n",
        "    def getNumericalOutputClass(dx) :\n",
        "        return np.where(outputClasses == dx)[0][0]\n",
        "    hamdf[\"dx\"] = hamdf[\"dx\"].map(getNumericalOutputClass)\n",
        "\n",
        "    #normalize age\n",
        "    hamdf[\"age\"]=(hamdf[\"age\"]-hamdf[\"age\"].min())/(hamdf[\"age\"].max()-hamdf[\"age\"].min())\n",
        "\n",
        "    hamdf = hamdf.drop([\"localization\", \"image_id\"], axis=1)    \n",
        "    return hamdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brdrwutef_sP"
      },
      "outputs": [],
      "source": [
        "#Gets a dataframe with ouly the path to the image and the output class dx\n",
        "def getHamDataFrameWithImagePathOnly(sample_fraction):\n",
        "    #gets a dataframe with a sample of the dataset\n",
        "    hamdf = getSampleOfData(sample_fraction)[[\"dx\",\"image_id\"]]\n",
        "    # get path to each image\n",
        "    def getImagePath(image_id) :\n",
        "        return datapath + \"/HAM10000_images/\" + image_id +\".jpg\"\n",
        "    hamdf[\"image_id\"] = hamdf[\"image_id\"].map(getImagePath)\n",
        "    #get output classes as numerical values\n",
        "    def getNumericalOutputClass(dx) :\n",
        "        return np.where(outputClasses == dx)[0][0]\n",
        "    hamdf[\"dx\"] = hamdf[\"dx\"].map(getNumericalOutputClass)\n",
        "    return hamdf\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvCqzoL1f_sQ"
      },
      "outputs": [],
      "source": [
        "# mutates and returns a hamdf with more equal amount of data points in each output class\n",
        "import random \n",
        "def getMoreEqualSample(hamdf):\n",
        "    def setLargestValuesNA(value):\n",
        "        if(value == largestvalue and random.randint(0, 3) < 1):\n",
        "            return None\n",
        "        return value\n",
        "    while(hamdf[\"dx\"].value_counts().nsmallest(1)[0] * 6 <  hamdf[\"dx\"].value_counts().nlargest(1).reset_index().iloc(0)[0][1]) :\n",
        "        largestvalue = hamdf[\"dx\"].value_counts().index[0]\n",
        "        hamdf[\"dx\"] = hamdf[\"dx\"].map(setLargestValuesNA)\n",
        "        hamdf = hamdf.dropna()\n",
        "    return hamdf.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6ZSIhdOf_sQ"
      },
      "outputs": [],
      "source": [
        "# adds color variance and color mean features to data frame \n",
        "# Mutates dataframe does not return a new one\n",
        "def addColorFeatures(hamdf):\n",
        "    def getColorValue(image_data):\n",
        "        return np.array(image_data).T[currentColor]\n",
        "    currentColor = 0\n",
        "    hamdf[\"red_values\"] = hamdf[\"image_data\"].map(getColorValue)\n",
        "    currentColor = 1\n",
        "    hamdf[\"green_values\"] = hamdf[\"image_data\"].map(getColorValue)\n",
        "    currentColor = 2\n",
        "    hamdf[\"blue_values\"] = hamdf[\"image_data\"].map(getColorValue)\n",
        "    #now creating custom features \n",
        "    #get Variance of all colors\n",
        "    def getColorVariance(color_data):\n",
        "        return np.var(color_data)\n",
        "    hamdf[\"red_var\"] = hamdf[\"red_values\"].map(getColorVariance)\n",
        "    hamdf[\"green_var\"] = hamdf[\"green_values\"].map(getColorVariance)\n",
        "    hamdf[\"blue_var\"] = hamdf[\"blue_values\"].map(getColorVariance)\n",
        "\n",
        "    #get Mean of all Colors\n",
        "    def getColorMean(color_data):\n",
        "        return np.mean(color_data)\n",
        "    hamdf[\"red_mean\"] = hamdf[\"red_values\"].map(getColorMean)\n",
        "    hamdf[\"green_mean\"] = hamdf[\"green_values\"].map(getColorMean)\n",
        "    hamdf[\"blue_mean\"] = hamdf[\"blue_values\"].map(getColorMean)\n",
        "    # remove repeated values\n",
        "    hamdf = hamdf.drop([\"red_values\", \"green_values\", \"blue_values\"], axis=1) \n",
        "    return hamdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q09nBtLf_sR"
      },
      "outputs": [],
      "source": [
        "# turns dx the output classes into binary classes based on whether the underlying condition is deadly \n",
        "# Mutates dataframe does not return a new one\n",
        "def getBinaryOutputClasses(hamdf):\n",
        "    # change to binary output\n",
        "    def binaryOuput(dx):\n",
        "        return int(dx > 3)\n",
        "    hamdf[\"dx\"] = hamdf[\"dx\"].map(binaryOuput)\n",
        "    return hamdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TECE2zEYf_sR"
      },
      "outputs": [],
      "source": [
        "# get train, test and validation set split (already in random order from sample)\n",
        "def getTrainTestVal(df) :    \n",
        "    train_set = df[:int(len(df)*.7)].reset_index()\n",
        "    test_set = df[int(len(df)*.7):int(len(df)*.9)].reset_index()\n",
        "    validation_set = df[int(len(df)*.9):].reset_index()\n",
        "    return train_set, test_set, validation_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eujjth4nf_sS"
      },
      "outputs": [],
      "source": [
        "#used to get a input vector [age, sex, location class,imagedata] and output vector [underlying condtion class] from of a row of the above dataframe \n",
        "def getInputNOutput(item):\n",
        "    x = []\n",
        "    x.append(int(item[\"age\"]))\n",
        "    # 1 is male 0 is female\n",
        "    x.append(int(item[\"sex\"] == \"male\"))\n",
        "    x = np.array(x)\n",
        "    #adds the location class\n",
        "    x = np.concatenate([x,getClassVector(item[\"localization\"], locClasses)])\n",
        "    #adds image classes\n",
        "    imgdata = np.concatenate([x,getHamImageAsVector(item[\"image_id\"])])\n",
        "    #get classification output class which is the underlying condtion \n",
        "    y = getClassVector(item[\"dx\"], outputClasses)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNCUb3dFf_sS"
      },
      "outputs": [],
      "source": [
        "# returns data set with input features as a flat vector and with the output as a scalar \n",
        "def getDataSets( df):\n",
        "    x = []\n",
        "    y = []\n",
        "    for index, row in df.iterrows():\n",
        "        row = np.array(row)\n",
        "        xi = []\n",
        "        for i in row[2:]:\n",
        "            xi = np.hstack([xi,np.array([i]).flatten()])\n",
        "        x.append(xi)\n",
        "        #print(row)\n",
        "        y.append([row[1]])\n",
        "    return np.array(x), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQwDWY52tqt"
      },
      "source": [
        "# LOGISTIC REGRESSION AND SUPPORT VECTOR MACHINES SECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SST-xRq5f_sS",
        "outputId": "14e677f8-b1c5-466f-fc13-093e6729abc1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9ddcaa74-5385-4dd8-b730-ce92b5b7e104\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dx</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>neck</th>\n",
              "      <th>lower extremity</th>\n",
              "      <th>back</th>\n",
              "      <th>trunk</th>\n",
              "      <th>abdomen</th>\n",
              "      <th>face</th>\n",
              "      <th>upper extremity</th>\n",
              "      <th>foot</th>\n",
              "      <th>genital</th>\n",
              "      <th>unknown</th>\n",
              "      <th>scalp</th>\n",
              "      <th>chest</th>\n",
              "      <th>hand</th>\n",
              "      <th>ear</th>\n",
              "      <th>acral</th>\n",
              "      <th>image_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>6</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[183, 137, 136], [190, 148, 153], [212, 174, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8103</th>\n",
              "      <td>2</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[174, 137, 147], [175, 136, 140], [176, 137, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9388</th>\n",
              "      <td>2</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[224, 212, 221], [225, 209, 220], [226, 210, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3148</th>\n",
              "      <td>2</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[226, 153, 162], [228, 153, 168], [227, 150, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5741</th>\n",
              "      <td>2</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[191, 112, 124], [188, 112, 130], [171, 99, 1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ddcaa74-5385-4dd8-b730-ce92b5b7e104')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ddcaa74-5385-4dd8-b730-ce92b5b7e104 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ddcaa74-5385-4dd8-b730-ce92b5b7e104');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      dx       age  ...  acral                                         image_data\n",
              "2763   6  0.529412  ...      0  [[183, 137, 136], [190, 148, 153], [212, 174, ...\n",
              "8103   2  0.294118  ...      0  [[174, 137, 147], [175, 136, 140], [176, 137, ...\n",
              "9388   2  0.588235  ...      0  [[224, 212, 221], [225, 209, 220], [226, 210, ...\n",
              "3148   2  0.588235  ...      0  [[226, 153, 162], [228, 153, 168], [227, 150, ...\n",
              "5741   2  0.352941  ...      0  [[191, 112, 124], [188, 112, 130], [171, 99, 1...\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#change the sample number input to help memory and run time\n",
        "hamdf = getHamDataFrame(sample_fraction)\n",
        "hamdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKKCTcRNf_sT",
        "outputId": "b5da75d4-a3d7-429f-ae79-e3fdbc7aa090"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['dx', 'age', 'sex', 'neck', 'lower extremity', 'back', 'trunk',\n",
              "       'abdomen', 'face', 'upper extremity', 'foot', 'genital', 'unknown',\n",
              "       'scalp', 'chest', 'hand', 'ear', 'acral', 'image_data'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "allFeatures = hamdf.columns\n",
        "allFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4ExiEBVf_sT",
        "outputId": "b43e8724-b3a7-4e82-c3e5-0b95fa43326c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2    5361\n",
              "1     718\n",
              "4     613\n",
              "6     327\n",
              "5     228\n",
              "3      98\n",
              "0      73\n",
              "Name: dx, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# counts of the output classes\n",
        "hamdf[\"dx\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGObO0mcf_sU"
      },
      "outputs": [],
      "source": [
        "#use this to det rid of warinings on the fact that it always wants more iterations for convergence \n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuhfhQ-tf_sU"
      },
      "outputs": [],
      "source": [
        "# gets a new dataframe that is less skewed towards any one class\n",
        "less_skewed_hamdf = getMoreEqualSample(hamdf.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIfsnXfPf_sU",
        "outputId": "411ebe1f-d062-47d1-df75-9a4e329ccfb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    410\n",
              "2.0    396\n",
              "4.0    335\n",
              "6.0    327\n",
              "5.0    228\n",
              "3.0     98\n",
              "0.0     73\n",
              "Name: dx, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "less_skewed_hamdf[\"dx\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--se4M9lf_sV"
      },
      "outputs": [],
      "source": [
        "#returns classification error\n",
        "def getCError(data, predictions):\n",
        "    correct = 0\n",
        "    for i in range(len(data[0])):\n",
        "        correct = correct + int(predictions[i] == data[1][i])\n",
        "    return (1 - (correct / len(data[0]))) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8r96Ukkf_sV"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression Training Testing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def LogisticRegressionTrainTest(train_data,test_data,val_data):\n",
        "    regularization_strengths = [.1,1,10,100]\n",
        "    best_error = 101\n",
        "    best_model = \"\"\n",
        "    for regularization_strength in regularization_strengths:\n",
        "        model = LogisticRegression(random_state=0,C=regularization_strength,solver='lbfgs').fit(train_data[0], train_data[1].ravel())\n",
        "        error = getCError(test_data, model.predict(test_data[0]))\n",
        "        if(error < best_error) :\n",
        "            best_error = error\n",
        "            best_model  = model\n",
        "    print(\"classification error for Logistic Regression training set = \",getCError(train_data, model.predict(train_data[0])), \"%\")\n",
        "    print(\"classification error for Logistic Regression test set = \",getCError(test_data, model.predict(test_data[0])), \"%\")\n",
        "    print(\"classification error for Logistic Regression validation set = \",getCError(val_data, model.predict(val_data[0])), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n71CyblOf_sV"
      },
      "outputs": [],
      "source": [
        "#Linear Support Vector Machine Training and Testing\n",
        "from sklearn.svm import LinearSVC\n",
        "def LinearSVMTrainTest(train_data,test_data,val_data):\n",
        "    regularization_strengths = [.1,1,10,100]\n",
        "    best_error = 101\n",
        "    for regularization_strength in regularization_strengths:\n",
        "    \n",
        "        model = LinearSVC(random_state=0, C=regularization_strength).fit(train_data[0], train_data[1].ravel())\n",
        "        error = getCError(test_data, model.predict(test_data[0]))\n",
        "        if(error < best_error) :\n",
        "            best_error = error\n",
        "            best_model  = model\n",
        "    print(\"classification error for Linear SVM training set = \",getCError(train_data, model.predict(train_data[0])), \"%\")\n",
        "    print(\"classification error for Linear SVM test set = \",getCError(test_data, model.predict(test_data[0])), \"%\")\n",
        "    print(\"classification error for Linear SVM validation set = \",getCError(val_data, model.predict(val_data[0])), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbXCihUef_sV"
      },
      "outputs": [],
      "source": [
        "#General Support Vector Machine Training and Testing\n",
        "from sklearn.svm import SVC\n",
        "def GeneralSVMTrainTest(train_data,test_data,val_data):\n",
        "    kernels = [\"poly\", \"rbf\", \"sigmoid\"]\n",
        "    best_error = 101\n",
        "    best_model = \"\"\n",
        "    for kernel in kernels:\n",
        " \n",
        "        model = SVC(C=10.0, kernel=kernel, random_state=0, shrinking=True).fit(train_data[0], train_data[1].ravel())\n",
        "        error = getCError(test_data, model.predict(test_data[0]))\n",
        "        if(error < best_error) :\n",
        "            best_error = error\n",
        "            best_model  = model\n",
        "    \n",
        "    print(\"classification error for General SVM training set = \",getCError(train_data, best_model.predict(train_data[0])), \"%\")\n",
        "    print(\"classification error for General SVM test set = \",getCError(test_data, best_model.predict(test_data[0])), \"%\")\n",
        "    print(\"classification error for General SVM validation_set = \",getCError(val_data, best_model.predict(val_data[0])), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd4wMHoRf_sW"
      },
      "outputs": [],
      "source": [
        "#trains and tests Logistic regression and Support Vector Machines with diffrent hyperparameters \n",
        "def trainTestLRSVM(hamdf):\n",
        "    train_set, test_set, validation_set = getTrainTestVal(hamdf)\n",
        "    train_data = getDataSets(train_set)\n",
        "    test_data = getDataSets(test_set)\n",
        "    val_data = getDataSets(validation_set)\n",
        "    \n",
        "    LogisticRegressionTrainTest(train_data,test_data,val_data)\n",
        "    LinearSVMTrainTest(train_data,test_data,val_data)\n",
        "    GeneralSVMTrainTest(train_data,test_data,val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4e6cNzPf_sW",
        "outputId": "3cc233bd-72d8-427d-b03d-3d20f832651e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification error for Logistic Regression training set =  23.998459167950692 %\n",
            "classification error for Logistic Regression test set =  26.07816711590296 %\n",
            "classification error for Logistic Regression validation set =  28.706199460916437 %\n",
            "classification error for Linear SVM training set =  8.532357473035434 %\n",
            "classification error for Linear SVM test set =  28.4366576819407 %\n",
            "classification error for Linear SVM validation set =  27.49326145552561 %\n",
            "classification error for General SVM training set =  5.816640986132516 %\n",
            "classification error for General SVM test set =  21.832884097035045 %\n",
            "classification error for General SVM validation_set =  23.180592991913752 %\n"
          ]
        }
      ],
      "source": [
        "# this section is just to show how  the logistic regression and SVM do without any feature manipulation\n",
        "trainTestLRSVM(hamdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fh0gc_m7f_sW",
        "outputId": "6f2d8ccc-9155-481c-b383-8cd832f60bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification error for Logistic Regression training set =  39.66309341500766 %\n",
            "classification error for Logistic Regression test set =  57.2192513368984 %\n",
            "classification error for Logistic Regression validation set =  45.98930481283422 %\n",
            "classification error for Linear SVM training set =  14.548238897396626 %\n",
            "classification error for Linear SVM test set =  63.36898395721925 %\n",
            "classification error for Linear SVM validation set =  51.33689839572193 %\n",
            "classification error for General SVM training set =  4.670750382848388 %\n",
            "classification error for General SVM test set =  45.98930481283422 %\n",
            "classification error for General SVM validation_set =  45.45454545454546 %\n"
          ]
        }
      ],
      "source": [
        "#now do the same but with data less skewed\n",
        "trainTestLRSVM(less_skewed_hamdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e3r2t8_1f_sW",
        "outputId": "0a8c47a2-98b1-4811-f124-a5e5213f4932"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-12403740-d54a-44e6-af1d-27331090f017\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dx</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>neck</th>\n",
              "      <th>lower extremity</th>\n",
              "      <th>back</th>\n",
              "      <th>trunk</th>\n",
              "      <th>abdomen</th>\n",
              "      <th>face</th>\n",
              "      <th>upper extremity</th>\n",
              "      <th>foot</th>\n",
              "      <th>genital</th>\n",
              "      <th>unknown</th>\n",
              "      <th>scalp</th>\n",
              "      <th>chest</th>\n",
              "      <th>hand</th>\n",
              "      <th>ear</th>\n",
              "      <th>acral</th>\n",
              "      <th>image_data</th>\n",
              "      <th>red_var</th>\n",
              "      <th>green_var</th>\n",
              "      <th>blue_var</th>\n",
              "      <th>red_mean</th>\n",
              "      <th>green_mean</th>\n",
              "      <th>blue_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>6</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[183, 137, 136], [190, 148, 153], [212, 174, ...</td>\n",
              "      <td>313.420837</td>\n",
              "      <td>824.873775</td>\n",
              "      <td>855.493453</td>\n",
              "      <td>212.304688</td>\n",
              "      <td>170.314453</td>\n",
              "      <td>174.678711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8103</th>\n",
              "      <td>2</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[174, 137, 147], [175, 136, 140], [176, 137, ...</td>\n",
              "      <td>602.308722</td>\n",
              "      <td>820.310303</td>\n",
              "      <td>1083.954575</td>\n",
              "      <td>170.864258</td>\n",
              "      <td>128.390625</td>\n",
              "      <td>126.183594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9388</th>\n",
              "      <td>2</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[224, 212, 221], [225, 209, 220], [226, 210, ...</td>\n",
              "      <td>100.803188</td>\n",
              "      <td>498.742942</td>\n",
              "      <td>949.542502</td>\n",
              "      <td>228.869141</td>\n",
              "      <td>195.668945</td>\n",
              "      <td>215.206055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3148</th>\n",
              "      <td>2</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[226, 153, 162], [228, 153, 168], [227, 150, ...</td>\n",
              "      <td>663.335934</td>\n",
              "      <td>909.795653</td>\n",
              "      <td>1057.270019</td>\n",
              "      <td>214.498047</td>\n",
              "      <td>140.874023</td>\n",
              "      <td>144.749023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5741</th>\n",
              "      <td>2</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[191, 112, 124], [188, 112, 130], [171, 99, 1...</td>\n",
              "      <td>823.856430</td>\n",
              "      <td>1499.055603</td>\n",
              "      <td>1894.978848</td>\n",
              "      <td>182.621094</td>\n",
              "      <td>107.570312</td>\n",
              "      <td>116.275391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12403740-d54a-44e6-af1d-27331090f017')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12403740-d54a-44e6-af1d-27331090f017 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12403740-d54a-44e6-af1d-27331090f017');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      dx       age  sex  neck  ...     blue_var    red_mean  green_mean   blue_mean\n",
              "2763   6  0.529412    1     1  ...   855.493453  212.304688  170.314453  174.678711\n",
              "8103   2  0.294118    0     0  ...  1083.954575  170.864258  128.390625  126.183594\n",
              "9388   2  0.588235    1     0  ...   949.542502  228.869141  195.668945  215.206055\n",
              "3148   2  0.588235    0     0  ...  1057.270019  214.498047  140.874023  144.749023\n",
              "5741   2  0.352941    1     0  ...  1894.978848  182.621094  107.570312  116.275391\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# adds the color features to the dfs\n",
        "hamdf = addColorFeatures(hamdf)\n",
        "less_skewed_hamdf = addColorFeatures(less_skewed_hamdf)\n",
        "hamdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P-OgOYtAf_sW",
        "outputId": "cdc3fe43-bf02-485e-8528-c89d6d2eb7dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification error for Logistic Regression training set =  23.497688751926038 %\n",
            "classification error for Logistic Regression test set =  27.156334231805936 %\n",
            "classification error for Logistic Regression validation set =  27.223719676549862 %\n",
            "classification error for Linear SVM training set =  8.26271186440678 %\n",
            "classification error for Linear SVM test set =  31.940700808625333 %\n",
            "classification error for Linear SVM validation set =  29.64959568733153 %\n",
            "classification error for General SVM training set =  8.378274268104779 %\n",
            "classification error for General SVM test set =  22.169811320754718 %\n",
            "classification error for General SVM validation_set =  23.045822102425873 %\n"
          ]
        }
      ],
      "source": [
        "trainTestLRSVM(hamdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oY7wgGLef_sX",
        "outputId": "057690c8-7dc9-4b0c-f40d-ce96aa8deeac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification error for Logistic Regression training set =  39.43338437978561 %\n",
            "classification error for Logistic Regression test set =  54.010695187165766 %\n",
            "classification error for Logistic Regression validation set =  43.85026737967914 %\n",
            "classification error for Linear SVM training set =  4.287901990811638 %\n",
            "classification error for Linear SVM test set =  55.88235294117647 %\n",
            "classification error for Linear SVM validation set =  50.26737967914438 %\n",
            "classification error for General SVM training set =  9.800918836140893 %\n",
            "classification error for General SVM test set =  44.11764705882353 %\n",
            "classification error for General SVM validation_set =  42.24598930481284 %\n"
          ]
        }
      ],
      "source": [
        "#now do the same but with data less skewed\n",
        "trainTestLRSVM(less_skewed_hamdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DY7Ip0ROf_sX",
        "outputId": "164980b2-95ec-4708-effa-0383bff03f35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3dddb8a4-81e7-407f-95cb-fccb9233a364\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dx</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>neck</th>\n",
              "      <th>lower extremity</th>\n",
              "      <th>back</th>\n",
              "      <th>trunk</th>\n",
              "      <th>abdomen</th>\n",
              "      <th>face</th>\n",
              "      <th>upper extremity</th>\n",
              "      <th>foot</th>\n",
              "      <th>genital</th>\n",
              "      <th>unknown</th>\n",
              "      <th>scalp</th>\n",
              "      <th>chest</th>\n",
              "      <th>hand</th>\n",
              "      <th>ear</th>\n",
              "      <th>acral</th>\n",
              "      <th>image_data</th>\n",
              "      <th>red_var</th>\n",
              "      <th>green_var</th>\n",
              "      <th>blue_var</th>\n",
              "      <th>red_mean</th>\n",
              "      <th>green_mean</th>\n",
              "      <th>blue_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>1</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[183, 137, 136], [190, 148, 153], [212, 174, ...</td>\n",
              "      <td>313.420837</td>\n",
              "      <td>824.873775</td>\n",
              "      <td>855.493453</td>\n",
              "      <td>212.304688</td>\n",
              "      <td>170.314453</td>\n",
              "      <td>174.678711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8103</th>\n",
              "      <td>0</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[174, 137, 147], [175, 136, 140], [176, 137, ...</td>\n",
              "      <td>602.308722</td>\n",
              "      <td>820.310303</td>\n",
              "      <td>1083.954575</td>\n",
              "      <td>170.864258</td>\n",
              "      <td>128.390625</td>\n",
              "      <td>126.183594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9388</th>\n",
              "      <td>0</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[224, 212, 221], [225, 209, 220], [226, 210, ...</td>\n",
              "      <td>100.803188</td>\n",
              "      <td>498.742942</td>\n",
              "      <td>949.542502</td>\n",
              "      <td>228.869141</td>\n",
              "      <td>195.668945</td>\n",
              "      <td>215.206055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3148</th>\n",
              "      <td>0</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[226, 153, 162], [228, 153, 168], [227, 150, ...</td>\n",
              "      <td>663.335934</td>\n",
              "      <td>909.795653</td>\n",
              "      <td>1057.270019</td>\n",
              "      <td>214.498047</td>\n",
              "      <td>140.874023</td>\n",
              "      <td>144.749023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5741</th>\n",
              "      <td>0</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[[191, 112, 124], [188, 112, 130], [171, 99, 1...</td>\n",
              "      <td>823.856430</td>\n",
              "      <td>1499.055603</td>\n",
              "      <td>1894.978848</td>\n",
              "      <td>182.621094</td>\n",
              "      <td>107.570312</td>\n",
              "      <td>116.275391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dddb8a4-81e7-407f-95cb-fccb9233a364')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dddb8a4-81e7-407f-95cb-fccb9233a364 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dddb8a4-81e7-407f-95cb-fccb9233a364');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      dx       age  sex  neck  ...     blue_var    red_mean  green_mean   blue_mean\n",
              "2763   1  0.529412    1     1  ...   855.493453  212.304688  170.314453  174.678711\n",
              "8103   0  0.294118    0     0  ...  1083.954575  170.864258  128.390625  126.183594\n",
              "9388   0  0.588235    1     0  ...   949.542502  228.869141  195.668945  215.206055\n",
              "3148   0  0.588235    0     0  ...  1057.270019  214.498047  140.874023  144.749023\n",
              "5741   0  0.352941    1     0  ...  1894.978848  182.621094  107.570312  116.275391\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# changes dx the class lable to binary based on wether the underlying condtion is deadly\n",
        "hamdf = getBinaryOutputClasses(hamdf)\n",
        "less_skewed_hamdf = getBinaryOutputClasses(less_skewed_hamdf)\n",
        "hamdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T34t79y2f_sX",
        "outputId": "42cde201-c5ab-42c9-f799-0a4df4e9382c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification error for Logistic Regression training set =  12.268875192604 %\n",
            "classification error for Logistic Regression test set =  16.307277628032345 %\n",
            "classification error for Logistic Regression validation set =  17.38544474393531 %\n",
            "classification error for Linear SVM training set =  8.763482280431434 %\n",
            "classification error for Linear SVM test set =  17.31805929919138 %\n",
            "classification error for Linear SVM validation set =  16.442048517520213 %\n",
            "classification error for General SVM training set =  4.449152542372881 %\n",
            "classification error for General SVM test set =  13.477088948787063 %\n",
            "classification error for General SVM validation_set =  14.690026954177892 %\n"
          ]
        }
      ],
      "source": [
        "trainTestLRSVM(hamdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lcDd90Tff_sX",
        "outputId": "18e77005-23b3-4786-b6a8-a2814b37aaca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classification error for Logistic Regression training set =  21.286370597243486 %\n",
            "classification error for Logistic Regression test set =  37.700534759358284 %\n",
            "classification error for Logistic Regression validation set =  36.36363636363637 %\n",
            "classification error for Linear SVM training set =  12.55742725880551 %\n",
            "classification error for Linear SVM test set =  39.037433155080215 %\n",
            "classification error for Linear SVM validation set =  37.96791443850267 %\n",
            "classification error for General SVM training set =  6.738131699846861 %\n",
            "classification error for General SVM test set =  29.14438502673797 %\n",
            "classification error for General SVM validation_set =  26.7379679144385 %\n"
          ]
        }
      ],
      "source": [
        "#now do the same but with data less skewed\n",
        "trainTestLRSVM(less_skewed_hamdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7W-6oH-6Xgz"
      },
      "source": [
        "# BINARY SIMPLE NEURAL NETS SECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BlEZ7QzBf_sY"
      },
      "outputs": [],
      "source": [
        "# gest classification of Binary \n",
        "def getAccNPredictionsNN(x,y, model):\n",
        "    predictions = model(x)\n",
        "    points  = []\n",
        "    totalRight = 0\n",
        "    for i in range(len(predictions)):\n",
        "        if(predictions[i] < 0.5) :\n",
        "            points.append(np.append(x[i].numpy(), [y[i], -1]))\n",
        "            if(y[i] < 0.5):\n",
        "                totalRight = totalRight + 1\n",
        "        else:\n",
        "            points.append(np.append(x[i].numpy(), [y[i], 1]))\n",
        "            if(y[i] > 0.5):\n",
        "                totalRight = totalRight + 1\n",
        "    print(\"classification error = \",(1 - totalRight/len(predictions)) * 100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qX05wz3Ff_sY"
      },
      "outputs": [],
      "source": [
        "def feedForwardNeuralNetwork(numLayers,numNodes,activationFunction,x,y) :\n",
        "    model = getNeuralNetwork(numLayers,numNodes,activationFunction)\n",
        "    model = model.double()\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    ##out.backward(torch.randn(1, 10))\n",
        "    for i in range(1000) :\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        #if i % 100 == 99:\n",
        "        #    print(i, loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fh1YZ6f0f_sY"
      },
      "outputs": [],
      "source": [
        "def getNeuralNetwork(numLayers,numNodes,activationFunction):\n",
        "    od = OrderedDict() \n",
        "    for i in range(len(numNodes) -1):\n",
        "        od['linear' + str(i)] = torch.nn.Linear(numNodes[i], numNodes[i+1])\n",
        "        if(i < len(numNodes) - 2):\n",
        "            if(activationFunction == \"relu\"):\n",
        "                od['rel ru' + str(i)] = torch.nn.ReLU()\n",
        "            if(activationFunction == \"sig\"):\n",
        "                od['sig' + str(i)] = torch.nn.Sigmoid()\n",
        "            if(activationFunction == \"tanh\"):\n",
        "                od['Tanh' + str(i)] = torch.nn.Tanh()\n",
        "            if(activationFunction == \"identity activation\"):\n",
        "                #do nothing here \n",
        "                fun = \"this class\"\n",
        "    return nn.Sequential(od)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n1_sN24Af_sY"
      },
      "outputs": [],
      "source": [
        "def trainTestPlotNN(test_data,train_data, hiddenLayerNumNodes, numHiddenLayers, activationFunction):\n",
        "    \n",
        "    inputNum = len(train_data[0][0])\n",
        "    outputClassesNum = 1\n",
        "    numNodes = []\n",
        "    numNodes.append(inputNum)\n",
        "    \n",
        "    for i in range(numHiddenLayers):\n",
        "        numNodes.append(hiddenLayerNumNodes)\n",
        "    numNodes.append(outputClassesNum)\n",
        "    train_y = []\n",
        "    test_y = []\n",
        "    for yi in train_data[1]:\n",
        "        train_y.append(np.array(yi))\n",
        "    for yi in test_data[1]:\n",
        "        test_y.append(np.array(yi))    \n",
        "    test_y = np.array(test_y)\n",
        "    train_y = np.array(train_y)\n",
        "    \n",
        "    train_x = torch.from_numpy(train_data[0].astype(float))\n",
        "    train_y = torch.from_numpy(train_y.astype(float))\n",
        "    test_x = torch.from_numpy(test_data[0].astype(float))\n",
        "    test_y = torch.from_numpy(test_y.astype(float))\n",
        "    \n",
        "    model = feedForwardNeuralNetwork(numHiddenLayers + 2, numNodes ,activationFunction,train_x,train_y)\n",
        "    \n",
        "    print(\"Training Dataset -\")\n",
        "    getAccNPredictionsNN(train_x,train_y, model)\n",
        "    print(\"Test Dataset -\")\n",
        "    getAccNPredictionsNN(test_x,test_y, model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GGP21zfPf_sY"
      },
      "outputs": [],
      "source": [
        "# do Binary training with diffrent hyper parameters the NN are all simple fully connected NN\n",
        "train_set, test_set, validation_set = getTrainTestVal(hamdf)\n",
        "train_data = getDataSets(train_set)\n",
        "test_data = getDataSets(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F8KX4QBef_sZ",
        "outputId": "e551088d-3aa8-4aa5-f613-3bfc8ef4be78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  15.446841294298919 %\n",
            "Test Dataset -\n",
            "classification error =  16.0377358490566 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=10, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 10, 1, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7gjTtlEyf_sZ",
        "outputId": "b3cc96d1-f24e-46ae-e33d-30082edb8155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  14.714946070878277 %\n",
            "Test Dataset -\n",
            "classification error =  15.566037735849058 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=10, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 10, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VF383XXhf_sZ",
        "outputId": "aea7f9ba-9586-4113-f425-29df3497ff44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  15.446841294298919 %\n",
            "Test Dataset -\n",
            "classification error =  16.0377358490566 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=10, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru2): ReLU()\n",
              "  (linear3): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru3): ReLU()\n",
              "  (linear4): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru4): ReLU()\n",
              "  (linear5): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 10, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BkA6Wwglf_sZ",
        "outputId": "f1a28402-39f1-4184-9aee-3cb48ec1f3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  12.731124807396 %\n",
            "Test Dataset -\n",
            "classification error =  15.970350404312672 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=100, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 100, 1, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xGBLLwfGf_sZ",
        "outputId": "97667bb8-be28-4f1c-e3a5-e585ab958c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  13.328197226502315 %\n",
            "Test Dataset -\n",
            "classification error =  14.690026954177892 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=100, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 100, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ptke0cVmf_sZ",
        "outputId": "e48db1c3-5e6f-4009-f80d-1cc349344765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  8.513097072419107 %\n",
            "Test Dataset -\n",
            "classification error =  14.353099730458219 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=100, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru2): ReLU()\n",
              "  (linear3): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru3): ReLU()\n",
              "  (linear4): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru4): ReLU()\n",
              "  (linear5): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 100, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xQo2HP2zf_sa",
        "outputId": "0dff5f56-092f-4775-d3f1-ed1112806c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  14.695685670261938 %\n",
            "Test Dataset -\n",
            "classification error =  15.970350404312672 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=1000, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 1000, 1, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Z49h7ZJf_sa",
        "outputId": "15f845dd-9803-4f91-b2a3-164f66cb1ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  10.997688751926038 %\n",
            "Test Dataset -\n",
            "classification error =  15.161725067385445 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=1000, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 1000, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SY3BQev6f_sa",
        "outputId": "ec097f47-16b0-40ef-a33b-5cf2b45bf745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  5.1617873651771955 %\n",
            "Test Dataset -\n",
            "classification error =  14.892183288409699 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=1000, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru2): ReLU()\n",
              "  (linear3): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru3): ReLU()\n",
              "  (linear4): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru4): ReLU()\n",
              "  (linear5): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 1000, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wezgR7dDf_sa",
        "outputId": "a71223eb-4f99-4eee-9a7f-587d1e773d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  10.22727272727273 %\n",
            "Test Dataset -\n",
            "classification error =  16.57681940700808 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=1000, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#the following hyperparameters had the best result on test data after multiple iterations  \n",
        "validation_data = getDataSets(validation_set)\n",
        "trainTestPlotNN(validation_data,train_data, 1000, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y31tyD6Ff_sa"
      },
      "outputs": [],
      "source": [
        "#now do the same but with data less skewed\n",
        "train_set, test_set, validation_set = getTrainTestVal(less_skewed_hamdf)\n",
        "train_data = getDataSets(train_set)\n",
        "test_data = getDataSets(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TkmCDuYPf_sa",
        "outputId": "d9027497-92ed-463c-b2f1-a5309d92d068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  29.862174578866775 %\n",
            "Test Dataset -\n",
            "classification error =  37.96791443850267 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=10, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 10, 1, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GIHzCSCsf_sa",
        "outputId": "46d095da-4934-43a5-9e57-8923d6807d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  24.11944869831547 %\n",
            "Test Dataset -\n",
            "classification error =  34.491978609625676 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=10, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 10, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dIQsR9_2f_sb",
        "outputId": "3547926b-036d-427b-bef3-64f05ba00ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  20.444104134762632 %\n",
            "Test Dataset -\n",
            "classification error =  33.68983957219251 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=10, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru2): ReLU()\n",
              "  (linear3): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru3): ReLU()\n",
              "  (linear4): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (rel ru4): ReLU()\n",
              "  (linear5): Linear(in_features=10, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 10, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "atE7drtkf_sb",
        "outputId": "7cea6a52-dc10-4641-da9a-bb54af9ae5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  29.32618683001531 %\n",
            "Test Dataset -\n",
            "classification error =  35.82887700534759 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=100, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 100, 1, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1FFyXtuWf_sb",
        "outputId": "4ae08ec5-002a-470d-9952-5a06ba2decd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  19.601837672281775 %\n",
            "Test Dataset -\n",
            "classification error =  36.36363636363637 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=100, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 100, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wh4ZL8gyf_sb",
        "outputId": "c23e2bc4-de01-4202-886f-83af69968663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  8.805513016845335 %\n",
            "Test Dataset -\n",
            "classification error =  33.42245989304813 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=100, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru2): ReLU()\n",
              "  (linear3): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru3): ReLU()\n",
              "  (linear4): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (rel ru4): ReLU()\n",
              "  (linear5): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 100, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fPyv5edYf_sb",
        "outputId": "67249c6b-4d13-48e0-b847-fef357a8e432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  33.154670750382856 %\n",
            "Test Dataset -\n",
            "classification error =  40.106951871657756 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=1000, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 1000, 1, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4YmI0Oojf_sb",
        "outputId": "cd1fda35-e3c3-40a6-8f66-2cd1bd015253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset -\n",
            "classification error =  7.810107197549765 %\n",
            "Test Dataset -\n",
            "classification error =  32.62032085561497 %\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear0): Linear(in_features=3095, out_features=1000, bias=True)\n",
              "  (rel ru0): ReLU()\n",
              "  (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (rel ru1): ReLU()\n",
              "  (linear2): Linear(in_features=1000, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainTestPlotNN(test_data,train_data, 1000, 2, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDAnWYctf_sc"
      },
      "outputs": [],
      "source": [
        "trainTestPlotNN(test_data,train_data, 1000, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEFC-9hLf_sc"
      },
      "outputs": [],
      "source": [
        "#the following hyperparameters had the best result on test data after multiple iterations \n",
        "validation_data = getDataSets(validation_set)\n",
        "trainTestPlotNN(validation_data,train_data, 100, 5, \"relu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYPUCVus6fAT"
      },
      "source": [
        "# CUSTOM CONVOLUTIONAL NEURAL NETWORK SECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "latZgE09f_sc"
      },
      "outputs": [],
      "source": [
        "#starter code for this section such as HAM1 network architecture gotten from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "#gets a dataframe with a sample of the dataset\n",
        "\n",
        "train_set, test_set, validation_set = getTrainTestVal(hamdf)\n",
        "\n",
        "class HAMDATALOADER(Dataset):\n",
        "    def __init__(self, hamdf,transform):\n",
        "        self.hamdf = hamdf\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hamdf)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = Image.open(self.hamdf['image_id'][index])\n",
        "        y = torch.tensor(int(self.hamdf['dx'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppxf2Si0f_sc"
      },
      "outputs": [],
      "source": [
        "#gets test and train dataloaders\n",
        "def getDataLoaders(hamdf):\n",
        "    ham_transform = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor()])\n",
        "    \n",
        "    train_set, test_set, validation_set = getTrainTestVal(hamdf)\n",
        "    \n",
        "    trains_set = HAMDATALOADER(train_set, transform=ham_transform)\n",
        "    train_loader = DataLoader(trains_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "    tests_set = HAMDATALOADER(test_set.reset_index(), transform=ham_transform)\n",
        "    test_loader = DataLoader(tests_set, shuffle=False, num_workers=0)\n",
        "    validation_set = HAMDATALOADER(validation_set.reset_index(), transform=ham_transform)\n",
        "    validation_loader = DataLoader(validation_set, shuffle=False, num_workers=0)\n",
        "    return train_loader, test_loader, validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF6zaXAOf_sc"
      },
      "outputs": [],
      "source": [
        "#trains given NN with given trainloader\n",
        "def trainImageNN(train_loader,net, epochs=10): \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgy__cbSf_sd"
      },
      "outputs": [],
      "source": [
        "#gets the classification error of given neural net on given test set in the form of a loader\n",
        "def getClassificationError(test_loader, net, test_set_length):\n",
        "    totalcorrect = 0\n",
        "    dataiter = iter(test_loader)\n",
        "    for i in range(test_set_length):\n",
        "        image, label = dataiter.next()\n",
        "        output = net(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        #print(predicted[0])\n",
        "        if(predicted[0] == label[0]):\n",
        "            totalcorrect = totalcorrect+1\n",
        "    return (1-(totalcorrect/test_set_length)) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcjQmrbSf_sd"
      },
      "outputs": [],
      "source": [
        "# base line network arcitecture from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "class HamNetOne(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HamNetOne, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4qmpcQxf_sd"
      },
      "outputs": [],
      "source": [
        "# Larger network arcitecture\n",
        "class HamNetTwo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HamNetTwo, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
        "        self.fc1 = nn.Linear(128 * 5 * 5, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 512)\n",
        "        self.fc3 = nn.Linear(512, 80)\n",
        "        self.fc4 = nn.Linear(80, 40)\n",
        "        self.fc5 = nn.Linear(40, 20)\n",
        "        self.fc6 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLAEzud2f_sd"
      },
      "outputs": [],
      "source": [
        "# smaller network arcitecture\n",
        "class HamNetThree(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HamNetThree, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 4, 5)\n",
        "        self.fc1 = nn.Linear(4 * 5 * 5, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 4 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po1UlemPf_sd"
      },
      "outputs": [],
      "source": [
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetOne()\n",
        "trainImageNN(train_loader,net)\n",
        "print(\"classification error for HAMNET 1 on test set  = \",getClassificationError(test_loader, net, len(test_set)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_hncOUff_sd"
      },
      "outputs": [],
      "source": [
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetTwo()\n",
        "trainImageNN(train_loader,net)\n",
        "\n",
        "print(\"classification error for HAMNET 2 on test set  = \",getClassificationError(test_loader, net, len(test_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pYCrBctf_sd"
      },
      "outputs": [],
      "source": [
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetTwo()\n",
        "trainImageNN(train_loader,net)\n",
        "\n",
        "print(\"classification error for HAMNET 3 on test set  = \",getClassificationError(test_loader, net, len(test_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF5BU8jTf_se"
      },
      "outputs": [],
      "source": [
        "#All had the same error so just using the last one\n",
        "print(\"classification error for HAMNET on validation set  = \",getClassificationError(validation_loader, net, len(validation_loader)), \"%\")\n",
        "print(\"classification error for HAMNET on training set = \",getClassificationError(train_loader, net, len(train_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MQFqMD8f_se"
      },
      "outputs": [],
      "source": [
        "#now do the same but with data less skewed\n",
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "hamdf = getMoreEqualSample(hamdf)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetOne()\n",
        "trainImageNN(train_loader,net)\n",
        "print(\"classification error for HAMNET 1 on test  set  = \",getClassificationError(test_loader, net, len(test_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJrgvSSVf_se"
      },
      "outputs": [],
      "source": [
        "#now do the same but with data less skewed\n",
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "hamdf = getMoreEqualSample(hamdf)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetTwo()\n",
        "trainImageNN(train_loader,net)\n",
        "print(\"classification error for HAMNET 2 on test set = \",getClassificationError(test_loader, net, len(test_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zHL5JkTf_se"
      },
      "outputs": [],
      "source": [
        "#now do the same but with data less skewed\n",
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "hamdf = getMoreEqualSample(hamdf)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetThree()\n",
        "trainImageNN(train_loader,net,5)\n",
        "print(\"classification error for HAMNET 3 on test set  = \",getClassificationError(test_loader, net, len(test_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2AAgOJuf_se"
      },
      "outputs": [],
      "source": [
        "#test validation data set because this model had the best results\n",
        "hamdf = getHamDataFrameWithImagePathOnly(sample_fraction)\n",
        "hamdf = getMoreEqualSample(hamdf)\n",
        "train_loader, test_loader, validation_loader = getDataLoaders(hamdf)\n",
        "net = HamNetTwo()\n",
        "trainImageNN(train_loader,net)\n",
        "print(\"classification error for HAMNET 2 on validation set = \",getClassificationError(validation_loader, net, len(validation_loader)), \"%\")\n",
        "print(\"classification error for HAMNET 2 on training set = \",getClassificationError(train_loader, net, len(train_loader)), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWXSMipMf_sf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEgA4E1Nf_sf"
      },
      "outputs": [],
      "source": [
        "#THANK YOU SECTION\n",
        "########################################################################################################\n",
        "########################################################################################################\n",
        "########################################################################################################\n",
        "########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rmXbg0Zf_sf"
      },
      "outputs": [],
      "source": [
        "#THANK YOU FOR A GREAT SEMESTER STAY SAFE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Project DS4400 Thompson.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}